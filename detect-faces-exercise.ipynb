{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Detect and Analyze Faces With the Azure Face Service\n",
    "\n",
    "In this notebook, you explore some of the capabilities of the Azure Face service using the `FaceClient` object found in the Azure Cognitive Services Python SDK.\n",
    "\n",
    "> **Important**: Several cells in this notebook contain `TODO` statements, where you need to update variables or enter code to enable the cell to execute without errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the notebook\n",
    "\n",
    "### Install the Azure Cognitive Services Face Library\n",
    "\n",
    "To access the Face service from this Python notebook, you need to install the Azure Cognitive Services Face Library. This library is part of the [Azure SDK for Python](https://github.com/Azure/azure-sdk-for-python) GitHub project.\n",
    "\n",
    "> To learn more, read the [Azure Cognitive Services modules for Python](https://docs.microsoft.com/python/api/overview/azure/cognitive-services?view=azure-python) article in Microsoft Docs.\n",
    "\n",
    "`TODO`: In the cell below, complete the `pip install` command to install the Azure Cognitive Services Face library, and then execute the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pip install azure-cognitiveservices-vision-computervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and services\n",
    "\n",
    "In addition to the Face library, we will also make use of a few other Python libraries in this notebook. Run the following cell to import the libraries and reference the services required to execute the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries need to call the Face service and execute the remaining cells in this notebook\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from py_code import faces # Custom Python code file located in py_code/faces.py.\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set variables\n",
    "\n",
    "To access your Azure Cognitive Services account, you must reference its authentication `key` and service `endpoint`.\n",
    "\n",
    "### Retrieve and set the `key` and `endpoint` values from your Azure Cognitive Services account.\n",
    "\n",
    "`TODO`: Using the values from the `Keys and Endpoints` page for you Cognitive Services account, replace the tokenized values in the cell below, as follows:\n",
    "\n",
    "- Retrieve the `Key 1` value for your cognitive services resource in the Azure portal and update the `key` value below.\n",
    "- Retrieve the `Endpoint` value for your cognitive services resource in the Azure portal and update the `endpoint` value below.\n",
    "\n",
    "> **Important**: You **must** replace the variable values below with the values from your Cognitive Services account or the remaining cells in this notebook will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update the variables below with values from your Cognitive Services account\n",
    "key = 'not authorized to expose'\n",
    "endpoint = 'not authorized to expose'\n",
    "\n",
    "print('Ready to analyze faces using the Azure Face service at {}.'.format(endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Face client\n",
    "\n",
    "After setting the `key` and `endpoint` values needed to access your Face service, you can instantiate a new client.\n",
    "\n",
    "`TODO`: Complete the code below to instantiate a new `FaceClient`, authenticating against your Azure Cogntive Services account, and then execute the cell below to create the `FaceClient` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Face client\n",
    "credentials = CognitiveServicesCredentials(key)\n",
    "client = ComputerVisionClient(endpoint, credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect faces\n",
    "\n",
    "`TODO`: In the cell below, locate the `TODO` statement (after `faceResults = client.`) and complete the line of code to call a `detect` operation of the `face` object that accepts an image data stream as an argument.\n",
    "\n",
    "> We covered this in the demo and lesson, but if you need help, you can learn more by reading the [FaceOperations documentation](https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-face/azure.cognitiveservices.vision.face.operations.faceoperations?view=azure-python#detect-with-stream-image--return-face-id-true--return-face-landmarks-false--return-face-attributes-none--recognition-model--recognition-01---return-recognition-model-false--detection-model--detection-01---custom-headers-none--raw-false--callback-none----operation-config-).\n",
    "\n",
    "Each detected face is assigned a unique ID, allowing your application to identify each individual face that was detected.\n",
    "\n",
    "After fixing the code on the `TODO` line, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Set the path to the image\n",
    "imagePath = os.path.join('images', 'family.jpg')\n",
    "\n",
    "# Detect faces\n",
    "with open(imagePath, mode=\"rb\") as imageData:\n",
    "    faceResults = client.faceResults = client.face.detect_with_stream(imageData)\n",
    "# Display the faces and output the Face Id for each detected face.\n",
    "faces.show_faces(imagePath, faceResults, show_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze facial attributes\n",
    "\n",
    "The Azure Face service is capable of much more than simply detect faces. Using the `return_face_attributes` argument, we can also analyze facial features and expressions. These attributes allow the Face service to predict approximate age and evaluate any emotional expressions present on the face.\n",
    "\n",
    "`TODO`: Locate the `TODO` statement in the cell below and complete the `client.face.detect_with_stream()` statement, passing in two arguments. The first argument should contain the stream data for the image, and the second should request that the Face service return the specified attributes about any faces in the image.\n",
    "\n",
    "After completing the `TODO`, execute the cell below to analyze the facial attributes of the women in our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open an image\n",
    "imagePath = os.path.join('images', 'child.jpg')\n",
    "\n",
    "# Detect faces and specified facial attributes\n",
    "attributes = ['age', 'gender', 'emotion']\n",
    "\n",
    "# Detect faces\n",
    "with open(imagePath, mode=\"rb\") as imageData:\n",
    "    faceResults = client.face.detect_with_stream(imageData, return_face_attributes=attributes)\n",
    "# Display the faces\n",
    "faces.show_faces(imagePath, faceResults, show_attributes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same code again, this time using a different face. We will also add in two additional facial attributes, `facial_hair` and `hair`.\n",
    "\n",
    "`TODO`: Make sure the update the `TODO` in this cell using the same code you came up with in the cell above to display the requested attributes, and then execute the cell below to examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open an image\n",
    "imagePath = os.path.join('images', 'man.jpg')\n",
    "\n",
    "# Detect faces and specified facial attributes\n",
    "attributes = ['age', 'gender', 'emotion', 'facialHair', 'hair']\n",
    "\n",
    "# Detect faces\n",
    "with open(imagePath, mode=\"rb\") as imageData:\n",
    "    faceResults = client.face.detect_with_stream(face_id=faceOne.face_id, face_ids=imageTwoFaceIds)\n",
    "# Display the faces\n",
    "faces.show_faces(imagePath, faceResults, show_attributes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similar faces\n",
    "\n",
    "When faces are detected using the Face service, each one is assigned a unique `Face Id`. Face Ids are used to individually identify face detections. You can use these Ifs to compare a detected face to previously detected faces and find faces with similar features.\n",
    "\n",
    "`TODO`: Complete the `similarFaces = client.` line near the bottom of the cell below, using a method of the Face service that allows you to find similar faces in two different images.\n",
    "\n",
    "Once the `TODO` code has been updated, run the cell below to compare the face in one image with the face in another, and check to see if it is matching face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths to the two images to compare\n",
    "imageOnePath = os.path.join('images', 'man-01.jpg')\n",
    "imageTwoPath = os.path.join('images', 'man-02.jpg')\n",
    "\n",
    "# Detect faces in the first image\n",
    "with open(imageOnePath, mode=\"rb\") as imageOneData:\n",
    "    faceOneResults = client.face.detect_with_stream(imageOneData)\n",
    "\n",
    "# Retrieve the first face identified in the image\n",
    "faceOne = faceOneResults[0]\n",
    "\n",
    "# Detect faces in the second image\n",
    "with open(imageTwoPath, mode=\"rb\") as imageTwoData:\n",
    "    faceTwoResults = client.face.detect_with_stream(imageTwoData)\n",
    "\n",
    "# Retrieve the face Ids found in the second image.\n",
    "imageTwoFaceIds = list(map(lambda face: face.face_id, faceTwoResults))\n",
    "\n",
    "# Find faces in image two that are similar to the face in image one\n",
    "similarFaces = client.face.find_similar(face_id=faceOne.face_id, face_ids=imageTwoFaceIds)\n",
    "# Show the face in image 1, and similar faces in image 2\n",
    "faces.show_similar_faces(imageOnePath, faceOne, imageTwoPath, faceTwoResults, similarFaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same code again, this time with two different faces and observe the results.\n",
    "\n",
    "`TODO`: Make sure the update the `TODO` in this cell using the same code you came up with in the cell above to compare similar faces between the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths to the two images to compare\n",
    "imageOnePath = os.path.join('images', 'man-03.jpg')\n",
    "imageTwoPath = os.path.join('images', 'man-05.jpg')\n",
    "\n",
    "# Detect faces in the first image\n",
    "with open(imageOnePath, mode=\"rb\") as imageOneData:\n",
    "    faceOneResults = client.face.detect_with_stream(imageOneData)\n",
    "\n",
    "# Retrieve the first face identified in the image\n",
    "faceOne = faceOneResults[0]\n",
    "\n",
    "# Detect faces in the second image\n",
    "with open(imageTwoPath, mode=\"rb\") as imageTwoData:\n",
    "    faceTwoResults = client.face.detect_with_stream(imageTwoData)\n",
    "\n",
    "# Retrieve the face Ids found in the second image.\n",
    "imageTwoFaceIds = list(map(lambda face: face.face_id, faceTwoResults))\n",
    "\n",
    "# Find faces in image two that are similar to the face in image one\n",
    "similarFaces = client.face.find_similar(face_id=faceOne.face_id, face_ids=imageTwoFaceIds)\n",
    "# Show the face in image 1, and similar faces in image 2\n",
    "faces.show_similar_faces(imageOnePath, faceOne, imageTwoPath, faceTwoResults, similarFaces)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
